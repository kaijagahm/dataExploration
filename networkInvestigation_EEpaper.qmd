---
title: "Network investigation (EcolEvol paper)"
format: html
editor: visual
---

## Drilling into how the network code works

```{r load-packages}
#| output: false
library(vultureUtils)
library(tidyverse)
library(sf)
```

For this investigation, I'm going to be working with data from December 2020 (2020-12-01) through June 2021 (2021-06-30), because that's the time interval that was used for the EE paper.

```{r define-dates}
minDate <- lubridate::ymd("2020-12-01")
maxDate <- lubridate::ymd("2021-06-30")
```

Let's begin by getting that data. Since we want to remove invalid periods anyway, and since my purpose here isn't to perfectly match the dataset that Nitika was using, I'm going to go ahead and use the dataset prepared in dataPrep.Rmd, which is datAnnotCleaned.Rda. I've recently updated this dataset to include data through the end of 2022, but that's not going to matter for this investigation, since I'll be subsetting it to the above dates anyway.

```{r load-subset-data}
#| output: false
load("data/datAnnotCleaned.Rda")
mask <- sf::st_read("data/CutOffRegion.kml")
roostPolygons <- sf::st_read("data/AllRoostPolygons.kml", 
                             quiet = TRUE) %>%
  sf::st_transform("WGS84")
subsetted <- datAnnotCleaned %>%
  filter(timestamp >= minDate & timestamp <= maxDate)
```

Now, using the functions in vultureUtils, create a co-flight edge list.

```{r}
# First, let's see what happens if we use getFlightEdges. 
gfe <- getFlightEdges(subsetted, roostPolygons = roostPolygons, quiet = T, includeAllVertices = F)
nrow(gfe) #1878 edges. This is what we're going for.

# Now, take the same arguments and use getEdges. This should give the same result, because getFlightEdges is just a wrapper on getEdges. If it doesn't, then I know there's something actually wrong with the functions in vultureUtils.
ge <- getEdges(subsetted, roostPolygons = roostPolygons, roostBuffer = 50, consecThreshold = 2, distThreshold = 1000, speedThreshUpper = NULL, speedThreshLower = 5, timeThreshold = "10 minutes", quiet = T, includeAllVertices = F)
nrow(ge) #1878 edges. Good, that lines up with gfe. Yay!
```

```{r}
# Now, in order to make sure I understand the guts of the code, I'm going to manually step through the function. Going to use getEdges, instead of getFeedingEdges, to eliminate one level of complexity (since I've already demonstrated that they're equivalent.)

# Define the arguments 
dataset <- subsetted
roostPolygons <- roostPolygons
roostBuffer <- 50
consecThreshold <- 2
distThreshold <- 1000
speedThreshUpper <- NULL
speedThreshLower <- 5
timeThreshold <- "10 minutes"
quiet <- T

# Argument checks
checkmate::assertDataFrame(dataset)
checkmate::assertClass(roostPolygons, "sf")
checkmate::assertNumeric(roostBuffer, len = 1)
checkmate::assertNumeric(consecThreshold, len = 1)
checkmate::assertNumeric(distThreshold, len = 1)
checkmate::assertNumeric(speedThreshUpper, len = 1, null.ok = TRUE)
checkmate::assertNumeric(speedThreshLower, len = 1, null.ok = TRUE)
checkmate::assertCharacter(timeThreshold, len = 1)

# Get all unique individuals before applying any filtering
if(includeAllVertices){
  uniqueIndivs <- unique(dataset$trackId)
}

# Restrict interactions based on ground speed
filteredData <- vultureUtils::filterLocs(df = dataset,
                                         speedThreshUpper = speedThreshUpper,
                                         speedThreshLower = speedThreshLower)

# Buffer the roost polygons
roostPolygonsBuffered <- convertAndBuffer(roostPolygons, dist = roostBuffer)

# Exclude any points that fall within a (buffered) roost polygon
points <- filteredData[lengths(sf::st_intersects(filteredData, roostPolygonsBuffered)) == 0,]

#If there are no rows left after filtering, return an empty data frame with the appropriate format.
if(nrow(points) == 0){
  dummy <- data.frame(timegroup = as.integer(),
                      ID1 = as.character(),
                      ID2 = as.character(),
                      distance = as.numeric(),
                      minTimestamp = as.POSIXct(character()),
                      maxTimestamp = as.POSIXct(character()))
  warning("After filtering, the dataset had 0 rows. Returning an empty edge list.")
  if(includeAllVertices){
    toReturn <- list("edges" = dummy,
                     "allVertices" = uniqueIndivs)
  }
}else{
  if(quiet == T){
    timestampCol <- "timestamp"
    crsToSet <- "WGS84"
    idCol <- "trackId"
    latCol <- "location_lat"
    longCol <- "location_long"
    returnDist <- T
    fillNA <- F
    # argument checks
    checkmate::assertDataFrame(points)
    checkmate::assertNumeric(distThreshold, len = 1, lower = 0, finite = TRUE)
    checkmate::assertNumeric(consecThreshold, len = 1, lower = 0)
    checkmate::assertCharacter(timestampCol, len = 1)
    checkmate::assertCharacter(timeThreshold, len = 1)
    checkmate::assertCharacter(idCol, len = 1)
    checkmate::assertCharacter(latCol, len = 1)
    checkmate::assertCharacter(longCol, len = 1)
    checkmate::assertLogical(returnDist, len = 1)
    checkmate::assertLogical(fillNA, len = 1)
    
    # Set up an sf object for use.
    if("sf" %in% class(points)){ # If dataset is an sf object...
      if(is.na(sf::st_crs(points))){ # only fill in crs if it is missing
        message(paste0("`points` is already an sf object but has no CRS. Setting CRS to ", crsToSet, "."))
        dataset <- sf::st_set_crs(points, crsToSet)
      }
    }else if(is.data.frame(points)){ # otherwise, if feedingSites is a data frame...
      # make sure it contains the lat and long cols
      checkmate::assertChoice(latCol, names(points))
      checkmate::assertChoice(longCol, names(points))
      
      if(nrow(points) == 0){
        stop("Dataset passed to vultureUtils::spaceTimeGroups has 0 rows. Cannot proceed with grouping.")
      }
      
      # convert to an sf object
      points <- points %>%
        sf::st_as_sf(coords = c(.data[[longCol]], .data[[latCol]]), remove = FALSE) %>%
        sf::st_set_crs(crsToSet) # assign the CRS
      
    }else{ # otherwise, throw an error.
      stop("`points` must be a data frame or an sf object.")
    }
    
    # Save lat and long coords, in case we need them later. Then, convert to UTM.
    points <- points %>%
      dplyr::mutate(lon = sf::st_coordinates(.)[,1],
                    lat = sf::st_coordinates(.)[,2]) %>%
      sf::st_transform(32636) %>% # convert to UTM: we'll need this for calculating distance later.
      dplyr::mutate(utmE = sf::st_coordinates(.)[,1],
                    utmN = sf::st_coordinates(.)[,2]) %>%
      sf::st_drop_geometry() # spatsoc won't work if this is still an sf object.
    
    # Convert the timestamp column to POSIXct.
    points <- points %>%
      dplyr::mutate({{timestampCol}} := as.POSIXct(.data[[timestampCol]], tz = "UTC"))
    
    # Convert to a data table for spatsoc.
    data.table::setDT(points)
    
    # Group the points into timegroups using spatsoc::group_times.
    points <- spatsoc::group_times(points, datetime = timestampCol, threshold = timeThreshold)
    timegroupData <- points %>% # save information about when each timegroup starts and ends.
      dplyr::select(.data[[timestampCol]], timegroup) %>%
      dplyr::group_by(timegroup) %>%
      dplyr::summarize(minTimestamp = min(.data[[timestampCol]]),
                       maxTimestamp = max(.data[[timestampCol]]))
    
    # Retain timestamps for each point, with timegroup information appending. This will be joined back at the end, to fix #43 and make individual points traceable.
    timestamps <- points %>%
      dplyr::select(.data[[timestampCol]], .data[[idCol]], timegroup)
    
    # Group into point groups (spatial)
    points <- spatsoc::group_pts(points, threshold = distThreshold, id = idCol,
                                 coords = c("utmE", "utmN"), timegroup = "timegroup")
    
    # Generate edge lists by timegroup
    edges <- spatsoc::edge_dist(DT = points, threshold = distThreshold, id = idCol,
                                coords = c("utmE", "utmN"), timegroup = "timegroup",
                                returnDist = returnDist, fillNA = fillNA)
    
    # Remove self and duplicate edges
    edges <- edges %>%
      dplyr::filter(as.character(.data$ID1) < as.character(.data$ID2))
    
    # Now create a list where the edge only stays if it occurred in at least `consecThreshold` consecutive time steps.
    # argument checks
    edgeList <- edges
    consecThreshold <- consecThreshold
    id1Col <- "ID1"
    id2Col <- "ID2"
    timegroupCol <- "timegroup"
    returnGroups <- F
    
    checkmate::assertDataFrame(edgeList)
    checkmate::assertNumeric(consecThreshold, len = 1)
    checkmate::assertCharacter(id1Col, len = 1)
    checkmate::assertCharacter(id2Col, len = 1)
    checkmate::assertChoice(id1Col, names(edgeList))
    checkmate::assertChoice(id2Col, names(edgeList))
    checkmate::assertCharacter(timegroupCol, len = 1)
    checkmate::assertChoice(timegroupCol, names(edgeList))
    checkmate::assertInteger(edgeList[[timegroupCol]])
    checkmate::assertLogical(returnGroups, len = 1)
    
    # do the filtering
    consec <- edgeList %>%
      # for each edge, arrange by timegroup
      dplyr::group_by(.data[[id1Col]], .data[[id2Col]]) %>%
      dplyr::arrange(.data[[timegroupCol]], .by_group = TRUE) %>%
      
      # create a new index grp that groups rows into consecutive runs
      dplyr::mutate("grp" = cumsum(c(1, diff(.data[[timegroupCol]]) != 1))) %>%
      dplyr::ungroup() %>%
      
      # group by the new `grp` column and remove any `grp`s that have less than `consecThreshold` rows (i.e. less than `consecThreshold` consecutive time groups for that edge)
      dplyr::group_by(.data[[id1Col]], .data[[id2Col]], .data$grp) %>%
      dplyr::filter(dplyr::n() >= consecThreshold) %>%
      dplyr::ungroup()
    
    # only return the group column if it's asked for.
    if(returnGroups == FALSE){
      consec <- consec %>%
        dplyr::ungroup() %>%
        dplyr::select(-.data$grp)
    }
    edgesFiltered <- consec
    
    # Join to the timegroup data
    edgesFiltered <- edgesFiltered %>%
      dplyr::left_join(timegroupData, by = "timegroup")
    
    # XXX need a step here where I join `timestamps` to `edgesFiltered`, in order to address #43. But for this to work, I have to decide what to do about the problem with some individuals showing up twice within the same 10-minute window.
    # Should I average their position during the window? Or should I pick just the first fix? Or should I compute the distance twice and if either of them is close enough to another individual, we consider it an edge? Very important to figure this out.
    edges <- edgesFiltered
  }else{
    edges <- vultureUtils::spaceTimeGroups(dataset = points,
                                           distThreshold = distThreshold,
                                           consecThreshold = consecThreshold,
                                           timeThreshold = timeThreshold)
  }
  if(includeAllVertices){
    toReturn <- list("edges" = edges,
                     "allVertices" = uniqueIndivs)
  }
}

nrow(edges) # okay, doing this gives us 1878 edges still. Hooray!

```



How many co-feeding, co-flight, co-roosting per month, total? Per individual? \*\*priority.

```{r}

```

For consecutive 10-minute slices, do they count as multiple or one

What is the duration of "an interaction"

Stop criterion: need to have stopped interacting for two consecutive 10-minute time slices?

When does the SRI correction happen? Does it happen for each ten-minute interval, or overall?

Do this first. When does she implement it?

For each project, what's the region that we include and which individuals are we including? Use those for SRI.

How does it deal with multiple interacting individuals?
