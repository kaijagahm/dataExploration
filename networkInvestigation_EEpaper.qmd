---
title: "Network investigation (EcolEvol paper)"
format: html
editor: visual
---

## Drilling into how the network code works

```{r load-packages}
#| output: false
library(vultureUtils)
library(tidyverse)
library(sf)
library(igraph)
```

For this investigation, I'm going to be working with data from December 2020 (2020-12-01) through June 2021 (2021-06-30), because that's the time interval that was used for the EE paper.

```{r define-dates}
minDate <- lubridate::ymd("2020-12-01")
maxDate <- lubridate::ymd("2021-06-30")
```

Let's begin by getting that data. Since we want to remove invalid periods anyway, and since my purpose here isn't to perfectly match the dataset that Nitika was using, I'm going to go ahead and use the dataset prepared in dataPrep.Rmd, which is datAnnotCleaned.Rda. I've recently updated this dataset to include data through the end of 2022, but that's not going to matter for this investigation, since I'll be subsetting it to the above dates anyway.

```{r load-subset-data}
#| output: false
#| echo: false
#| warning: false
#| message: false
load("data/datAnnotCleaned.Rda")
mask <- sf::st_read("data/CutOffRegion.kml")
roostPolygons <- sf::st_read("data/AllRoostPolygons.kml", 
                             quiet = TRUE) %>%
  sf::st_transform("WGS84")
subsetted <- datAnnotCleaned %>%
  filter(timestamp >= minDate & timestamp <= maxDate)
```

Now, using the functions in vultureUtils, create a co-flight edge list.

```{r}
#| echo: false
# First, let's see what happens if we use getFlightEdges. 
gfe <- getFlightEdges(subsetted, roostPolygons = roostPolygons, quiet = T, includeAllVertices = F)
nrow(gfe) #1878 edges. This is what we're going for.

# Now, take the same arguments and use getEdges. This should give the same result, because getFlightEdges is just a wrapper on getEdges. If it doesn't, then I know there's something actually wrong with the functions in vultureUtils.
ge <- getEdges(subsetted, roostPolygons = roostPolygons, roostBuffer = 50, consecThreshold = 2, distThreshold = 1000, speedThreshUpper = NULL, speedThreshLower = 5, timeThreshold = "10 minutes", quiet = T, includeAllVertices = F)
nrow(ge) #1878 edges. Good, that lines up with gfe. Yay!
```

```{r}
#| echo: false
#| output: false
# Now, in order to make sure I understand the guts of the code, I'm going to manually step through the function. Going to use getEdges, instead of getFeedingEdges, to eliminate one level of complexity (since I've already demonstrated that they're equivalent.)

# Define the arguments 
dataset <- subsetted
roostPolygons <- roostPolygons
roostBuffer <- 50
consecThreshold <- 2
distThreshold <- 1000
speedThreshUpper <- NULL
speedThreshLower <- 5
timeThreshold <- "10 minutes"
quiet <- T
includeAllVertices <- F

# Argument checks
checkmate::assertDataFrame(dataset)
checkmate::assertClass(roostPolygons, "sf")
checkmate::assertNumeric(roostBuffer, len = 1)
checkmate::assertNumeric(consecThreshold, len = 1)
checkmate::assertNumeric(distThreshold, len = 1)
checkmate::assertNumeric(speedThreshUpper, len = 1, null.ok = TRUE)
checkmate::assertNumeric(speedThreshLower, len = 1, null.ok = TRUE)
checkmate::assertCharacter(timeThreshold, len = 1)

# Get all unique individuals before applying any filtering
if(includeAllVertices){
  uniqueIndivs <- unique(dataset$trackId)
}

# Restrict interactions based on ground speed
filteredData <- vultureUtils::filterLocs(df = dataset,
                                         speedThreshUpper = speedThreshUpper,
                                         speedThreshLower = speedThreshLower)

# Buffer the roost polygons
roostPolygonsBuffered <- convertAndBuffer(roostPolygons, dist = roostBuffer)

# Exclude any points that fall within a (buffered) roost polygon
points <- filteredData[lengths(sf::st_intersects(filteredData, roostPolygonsBuffered)) == 0,]

#If there are no rows left after filtering, return an empty data frame with the appropriate format.
if(nrow(points) == 0){
  dummy <- data.frame(timegroup = as.integer(),
                      ID1 = as.character(),
                      ID2 = as.character(),
                      distance = as.numeric(),
                      minTimestamp = as.POSIXct(character()),
                      maxTimestamp = as.POSIXct(character()))
  warning("After filtering, the dataset had 0 rows. Returning an empty edge list.")
  if(includeAllVertices){
    toReturn <- list("edges" = dummy,
                     "allVertices" = uniqueIndivs)
  }
}else{
  if(quiet == T){
    timestampCol <- "timestamp"
    crsToSet <- "WGS84"
    idCol <- "trackId"
    latCol <- "location_lat"
    longCol <- "location_long"
    returnDist <- T
    fillNA <- F
    # argument checks
    checkmate::assertDataFrame(points)
    checkmate::assertNumeric(distThreshold, len = 1, lower = 0, finite = TRUE)
    checkmate::assertNumeric(consecThreshold, len = 1, lower = 0)
    checkmate::assertCharacter(timestampCol, len = 1)
    checkmate::assertCharacter(timeThreshold, len = 1)
    checkmate::assertCharacter(idCol, len = 1)
    checkmate::assertCharacter(latCol, len = 1)
    checkmate::assertCharacter(longCol, len = 1)
    checkmate::assertLogical(returnDist, len = 1)
    checkmate::assertLogical(fillNA, len = 1)
    
    # Set up an sf object for use.
    if("sf" %in% class(points)){ # If dataset is an sf object...
      if(is.na(sf::st_crs(points))){ # only fill in crs if it is missing
        message(paste0("`points` is already an sf object but has no CRS. Setting CRS to ", crsToSet, "."))
        dataset <- sf::st_set_crs(points, crsToSet)
      }
    }else if(is.data.frame(points)){ # otherwise, if feedingSites is a data frame...
      # make sure it contains the lat and long cols
      checkmate::assertChoice(latCol, names(points))
      checkmate::assertChoice(longCol, names(points))
      
      if(nrow(points) == 0){
        stop("Dataset passed to vultureUtils::spaceTimeGroups has 0 rows. Cannot proceed with grouping.")
      }
      
      # convert to an sf object
      points <- points %>%
        sf::st_as_sf(coords = c(.data[[longCol]], .data[[latCol]]), remove = FALSE) %>%
        sf::st_set_crs(crsToSet) # assign the CRS
      
    }else{ # otherwise, throw an error.
      stop("`points` must be a data frame or an sf object.")
    }
    
    # Save lat and long coords, in case we need them later. Then, convert to UTM.
    points <- points %>%
      dplyr::mutate(lon = sf::st_coordinates(.)[,1],
                    lat = sf::st_coordinates(.)[,2]) %>%
      sf::st_transform(32636) %>% # convert to UTM: we'll need this for calculating distance later.
      dplyr::mutate(utmE = sf::st_coordinates(.)[,1],
                    utmN = sf::st_coordinates(.)[,2]) %>%
      sf::st_drop_geometry() # spatsoc won't work if this is still an sf object.
    
    # Convert the timestamp column to POSIXct.
    points <- points %>%
      dplyr::mutate({{timestampCol}} := as.POSIXct(.data[[timestampCol]], tz = "UTC"))
    
    # Convert to a data table for spatsoc.
    data.table::setDT(points)
    
    # Group the points into timegroups using spatsoc::group_times.
    points_timegrouped <- spatsoc::group_times(points, datetime = timestampCol, threshold = timeThreshold)
    timegroupData <- points_timegrouped %>% # save information about when each timegroup starts and ends.
      dplyr::select(.data[[timestampCol]], timegroup) %>%
      dplyr::group_by(timegroup) %>%
      dplyr::summarize(minTimestamp = min(.data[[timestampCol]]),
                       maxTimestamp = max(.data[[timestampCol]]))
    
    # Retain timestamps for each point, with timegroup information appending. This will be joined back at the end, to fix #43 and make individual points traceable.
    timestamps <- points_timegrouped %>%
      dplyr::select(.data[[timestampCol]], .data[[idCol]], timegroup)
    
    # Group into point groups (spatial)
    points_spacetimegrouped <- spatsoc::group_pts(points_timegrouped, threshold = distThreshold, id = idCol,
                                 coords = c("utmE", "utmN"), timegroup = "timegroup")
    
    # Generate edge lists by timegroup
    edges <- spatsoc::edge_dist(DT = points_spacetimegrouped, threshold = distThreshold, id = idCol,
                                coords = c("utmE", "utmN"), timegroup = "timegroup",
                                returnDist = returnDist, fillNA = fillNA)
    
    # Remove self and duplicate edges
    edges <- edges %>%
      dplyr::filter(as.character(.data$ID1) < as.character(.data$ID2))
    
    # Now create a list where the edge only stays if it occurred in at least `consecThreshold` consecutive time steps.
    # argument checks
    edgeList <- edges
    consecThreshold <- consecThreshold
    id1Col <- "ID1"
    id2Col <- "ID2"
    timegroupCol <- "timegroup"
    returnGroups <- F
    
    checkmate::assertDataFrame(edgeList)
    checkmate::assertNumeric(consecThreshold, len = 1)
    checkmate::assertCharacter(id1Col, len = 1)
    checkmate::assertCharacter(id2Col, len = 1)
    checkmate::assertChoice(id1Col, names(edgeList))
    checkmate::assertChoice(id2Col, names(edgeList))
    checkmate::assertCharacter(timegroupCol, len = 1)
    checkmate::assertChoice(timegroupCol, names(edgeList))
    checkmate::assertInteger(edgeList[[timegroupCol]])
    checkmate::assertLogical(returnGroups, len = 1)
    
    # do the filtering
    consec <- edgeList %>%
      # for each edge, arrange by timegroup
      dplyr::group_by(.data[[id1Col]], .data[[id2Col]]) %>%
      dplyr::arrange(.data[[timegroupCol]], .by_group = TRUE) %>%
      
      # create a new index grp that groups rows into consecutive runs
      dplyr::mutate("grp" = cumsum(c(1, diff(.data[[timegroupCol]]) != 1))) %>%
      dplyr::ungroup() %>%
      
      # group by the new `grp` column and remove any `grp`s that have less than `consecThreshold` rows (i.e. less than `consecThreshold` consecutive time groups for that edge)
      dplyr::group_by(.data[[id1Col]], .data[[id2Col]], .data$grp) %>%
      dplyr::filter(dplyr::n() >= consecThreshold) %>%
      dplyr::ungroup()
    
    # only return the group column if it's asked for.
    if(returnGroups == FALSE){
      consec <- consec %>%
        dplyr::ungroup() %>%
        dplyr::select(-.data$grp)
    }
    edgesFiltered <- consec
    
    # Join to the timegroup data
    edgesFiltered <- edgesFiltered %>%
      dplyr::left_join(timegroupData, by = "timegroup")
    
    # XXX need a step here where I join `timestamps` to `edgesFiltered`, in order to address #43. But for this to work, I have to decide what to do about the problem with some individuals showing up twice within the same 10-minute window.
    # Should I average their position during the window? Or should I pick just the first fix? Or should I compute the distance twice and if either of them is close enough to another individual, we consider it an edge? Very important to figure this out.
    edges <- edgesFiltered
  }else{
    edges <- vultureUtils::spaceTimeGroups(dataset = points,
                                           distThreshold = distThreshold,
                                           consecThreshold = consecThreshold,
                                           timeThreshold = timeThreshold)
  }
  if(includeAllVertices){
    toReturn <- list("edges" = edges,
                     "allVertices" = uniqueIndivs)
  }
}

nrow(edges) # okay, doing this gives us 1878 edges still. Hooray!
```

How many co-feeding, co-flight, co-roosting per month, total?

```{r}
flight <- getFlightEdges(subsetted, roostPolygons = roostPolygons, quiet = T, includeAllVertices = F)
feeding <- getFeedingEdges(subsetted, roostPolygons = roostPolygons, quiet = T, includeAllVertices = F)

# Make graphs by month
flight <- flight %>%
  mutate(month = lubridate::month(minTimestamp),
         year = lubridate::year(minTimestamp),
         type = "flight")

feeding <- feeding %>%
  mutate(month = lubridate::month(minTimestamp),
         year = lubridate::year(minTimestamp),
         type = "feeding")

ff <- bind_rows(flight, feeding)

## plots
summ <- ff %>%
  group_by(type, month, year) %>%
  summarize(nEdges = n())

# Fixed y axis
summ %>%
  ggplot(aes(x = lubridate::ym(paste(year, month)), y = nEdges))+
  geom_col(aes(fill = type))+
  facet_wrap(~type)+
  theme_classic()+
  ggtitle("# interactions, timegroups separate")

# Free y axis
summ %>%
  ggplot(aes(x = lubridate::ym(paste(year, month)), y = nEdges))+
  geom_col(aes(fill = type))+
  facet_wrap(~type, scales = "free_y")+
  theme_classic()+
  ggtitle("# interactions, timegroups separate")
```

### For consecutive 10-minute slices, do they count as multiple or one?

```{r}
# I believe they're being counted as multiple interactions. For example, let's look at co-flight.
head(flight)

# Let's look at some from Noa's example screenshot.
indivs <- c("A03w", "A05w")
example <- flight %>%
  filter(ID1 %in% indivs & ID2 %in% indivs)

# does this include self edges? I think I have taken them out.
example %>% filter(ID1 == ID2) # good, there are none.

# does this include multiple during the same time slice?
example %>% group_by(timegroup) %>%
  summarize(n = n()) %>%
  filter(n > 1) # nope, I've removed all duplicate edges. This is as it should be.

# Consecutive time slices:
head(example) # as we can see, we have interactions in consecutive time slices. Those are treated as separate edges in the network.
``` 


### What is the duration of "an interaction"?

```{r}
#| echo: false
timegroupData_flight <- flight %>%
  select(timegroup, minTimestamp, maxTimestamp, month, year) %>%
  distinct()

timegroupData_feeding <- feeding %>%
  select(timegroup, minTimestamp, maxTimestamp, month, year) %>%
  distinct()

durations_flight <- flight %>%
  group_by(ID1, ID2) %>%
  arrange(timegroup, .by_group = T) %>%
  group_by(ID1, ID2) %>%
  mutate(diff = c(0, diff(timegroup) - 1)) %>%
  group_by(ID1, ID2, cumsum(diff)) %>%
  summarize(firstTimegroup = first(timegroup),
            lastTimegroup = last(timegroup),
            nTimegroups = n()) %>%
  ungroup() %>%
  left_join(timegroupData_flight %>% select(timegroup, "monthStart" = month, "yearStart" = year), by = c("firstTimegroup" = "timegroup")) %>%
  left_join(timegroupData_flight %>% select(timegroup, "monthEnd" = month, "yearEnd" = year), by = c("lastTimegroup" = "timegroup"))

head(durations_flight)

durations_feeding <- feeding %>%
  group_by(ID1, ID2) %>%
  arrange(timegroup, .by_group = T) %>%
  group_by(ID1, ID2) %>%
  mutate(diff = c(0, diff(timegroup) - 1)) %>%
  group_by(ID1, ID2, cumsum(diff)) %>%
  summarize(firstTimegroup = first(timegroup),
            lastTimegroup = last(timegroup),
            nTimegroups = n()) %>%
  ungroup() %>%
  left_join(timegroupData_feeding %>% select(timegroup, "monthStart" = month, "yearStart" = year), by = c("firstTimegroup" = "timegroup")) %>%
  left_join(timegroupData_feeding %>% select(timegroup, "monthEnd" = month, "yearEnd" = year), by = c("lastTimegroup" = "timegroup"))

head(durations_feeding)

# Keeping in mind that the minimum duration has to be 2 based on how consecThreshold was defined in our function, what's the distribution of durations of interactions?
durations <- durations_flight %>%
  mutate(type = "flight") %>%
  bind_rows(durations_feeding %>%
              mutate(type = "feeding"))
```

```{r}
# Show duration graphs
durations %>%
  ggplot(aes(x = nTimegroups*10, col = type, fill = type))+
  geom_histogram()+
  theme_classic()+
  ylab("Frequency")+
  xlab("Duration of interaction (minutes)")+
  facet_wrap(~type)

durations %>%
  ggplot(aes(x = nTimegroups*10, col = type))+
  geom_density(size = 1.5)+
  theme_classic()+
  ylab("Frequency")+
  xlab("Duration of interaction (minutes)")
```

### Investigate long durations (flight)

```{r}
#| echo: false
# Some of these seem unusually large. Let's investigate.
durations_flight %>%
  filter(nTimegroups > 10)
# hmm, that's suspicious. Why are all of these between the same two individuals?

durations_flight %>%
  filter(nTimegroups > 6) %>%
  arrange(nTimegroups)

# Looking at the longest one, between A00w and J12w
check <- flight %>%
  filter(timegroup >= 322 & timegroup <= 347 & ID1 == "A00w" & ID2 == "J12w")

# Extract the timestamps and go back to the data
mn <- min(check$minTimestamp)
mx <- max(check$maxTimestamp)

checkData1 <- subsetted %>% filter(trackId %in% c("A00w", "J12w"), timestamp >= mn, timestamp <= mx) %>%
  sf::st_as_sf(coords = c("location_long", "location_lat")) %>%
  sf::st_set_crs("WGS84")

mapview::mapview(checkData1, zcol = "trackId") # huh, so they seem to be flying together for a very long way, never more than a couple hundred meters apart. This strikes me as a bit suspicious. Do they actually fly that tightly together? Or was this a mis-recording of a single bird?

# Let me check another one that's not quite as long
durations_flight %>%
  filter(nTimegroups > 6) %>%
  arrange(nTimegroups)

# Looking at the longest one, between A00w and J12w
check <- flight %>%
  filter(timegroup >= 1007 & timegroup <= 1014 & ID1 == "T25b" & ID2 == "T76w")

# Extract the timestamps and go back to the data
mn <- min(check$minTimestamp)
mx <- max(check$maxTimestamp)

checkData2 <- subsetted %>% filter(trackId %in% c("T25b", "T76w"), timestamp >= mn, timestamp <= mx) %>%
  sf::st_as_sf(coords = c("location_long", "location_lat")) %>%
  sf::st_set_crs("WGS84")

mapview::mapview(checkData2, zcol = "trackId") # looks pretty similar to the last one. But two different individuals. Maybe they just do this periodically!
```

### \# Monthly interactions, grouping timegroups

```{r}
durations_summary <- durations %>%
  mutate(myStart = lubridate::my(paste(monthStart, yearStart))) %>%
  group_by(type, myStart) %>%
  summarize(nInteractions = n())

# absolute numbers, fixed y scale
durations_summary %>%
  ggplot(aes(x = myStart, y = nInteractions, fill = type))+
  geom_col()+
  facet_wrap(~type)+
  theme_classic()+
  ylab("Number of interactions")+
  xlab("Month of interaction start")+
  theme(legend.position = "none")+
  ggtitle("# interactions, timegroups separate")

# absolute numbers, free y scale
durations_summary %>%
  ggplot(aes(x = myStart, y = nInteractions, fill = type))+
  geom_col()+
  facet_wrap(~type, scales = "free_y")+
  theme_classic()+
  ylab("Number of interactions")+
  xlab("Month of interaction start")+
  theme(legend.position = "none")+ # basically the same pattern, just fewer flight interactions than feeding interactions.
  ggtitle("# interactions, timegroups separate")

# Duration of interactions, by month
head(durations)
durations %>%
  mutate(myStart = factor(lubridate::my(paste(monthStart, yearStart)))) %>%
  ggplot(aes(x = myStart, y = log(nTimegroups)))+
  geom_boxplot(aes(fill = type))+
  facet_wrap(~type)+
  theme_classic()+
  ylab("Number of timegroups (log-transformed)")+
  xlab("")+
  ggtitle("# interactions, timegroups separate")
```

### SRI

When does the SRI correction happen? Does it happen for each ten-minute interval, or overall?
Do this first. When does Nitika implement it in her code? [I just checked, and it looks like she does it at the end.]

For each project, what's the region that we include and which individuals are we including? Use those for SRI.
```{r}
#| include: false
# Okay, so Nitika defines SRI as the ratio of [number of timegroups in which both individuals interact (aka co-occurrences)]/[number of timegroups in which both individuals occur in the dataset but don't necessarily interact (simultaneous datapoints)].
# This seems a bit different from the definition of SRI found e.g. here: https://dshizuka.github.io/networkanalysis/networktypes_socialnetworks.html

# M1: adjacency matrix for interactions (aka spatio-temporal co-occurrence)
el <- flight %>%
  select(ID1, ID2)
g1 <- igraph::graph.data.frame(el, directed = F)
m1 <- get.adjacency(g1, sparse = F)
colnames(m1) <- row.names(m1)

# M2: adjacency matrix for temporal co-occurrence (but not spatial)
head(points_timegrouped)
el_m2 <- points_timegrouped %>%
  select(trackId, timegroup) %>%
  distinct()
timegroupList <- el_m2 %>%
  group_by(timegroup) %>%
  group_split()
timegroupEdgelist <- timegroupList %>%
  map_dfr(., ~.x %>% 
        pull(trackId) %>% 
        expand.grid(., ., stringsAsFactors = F))

# now this is tricky. We want to remove self edges and remove reverse edges, but we *don't* want to remove duplicates.
test <- timegroupEdgelist %>%
  filter(Var1 > Var2)
test %>%
  group_by(Var1, Var2) %>%
  summarize(n = n()) %>%
  filter(n > 1) # okay, good, there are still many that are greater than 1
test %>%
  filter(Var1 == Var2) # good, no self edges.

g2 <- igraph::graph.data.frame(timegroupEdgelist, directed = F)
m2 <- get.adjacency(g2, sparse = F)
colnames(m2) <- row.names(m2)
  

dim(m1)
dim(m2)

subset_m2 <- m2[row.names(m1), row.names(m1)]
dim(subset_m2)
dim(m1)

all(m1 <= subset_m2) # yay!!

sriMatrix <- m1/subset_m2
```


### Multiple interacting individuals

```{r}
# Here's an example of a timegroup that contains a lot of individuals
bigGroup <- flight %>%
  group_by(timegroup) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  filter(n == max(n, na.rm = T)) %>%
  filter(timegroup == timegroup[1])

# How many unique individuals?
length(unique(c(bigGroup$ID1, bigGroup$ID2))) # 10 unique individuals
nrow(bigGroup) # 22 edges during this timegroup

# Multiple interacting individuals are just given their own edges. This almost certainly accounts for why the feeding network has so many more edges than the flight network.
```

